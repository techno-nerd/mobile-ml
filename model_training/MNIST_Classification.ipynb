{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Neural Networks\n",
    "In this lesson, we will learn more about unstructured inputs: images.\n",
    "\n",
    "In this notebook, we will use a public dataset, the MNIST Digit data set. This dataset contains 60,000 images (28 x 28) of digits from 0-9.\n",
    "\n",
    "In addition, the test set contains 10,000 similar images.\n",
    "\n",
    "### Load and explore data\n",
    "Using a python package by tensorflow (keras), we can directly import the data into our local environment.\n",
    "\n",
    "We can use matplotlib to visualise some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKUAAAClCAYAAAA9Kz3aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFuklEQVR4nO3dX2iVdRzH8ee4s2bmUpdpCqbLPylqrpKaOTQIzYsuilgiXhldpKlYCywJ+sMKiwiWLS8EUyHLlCIv+kNEDCG1zDAscuE2wj+ttoOz1HKec7oNP7/0Uc/c5znn/br88JxnP+TjD7485/mdVD6fz0eAkQH9vQDgfJQSdigl7FBK2KGUsEMpYYdSwg6lhJ103AvnDajvy3WgRHyR237Ra9gpYYdSwg6lhB1KCTuUEnYoJexQStihlLBDKWGHUsIOpYQdSgk7lBJ2KCXsUErYoZSwQylhh1LCDqWEHUoJO5QSdigl7FBK2KGUsEMpYSf2CRmlLJXWf6ayG4df9v0OPT0umGcH5SQbO/53yQYtS0n22xvXSLZ/5rbg3+nKnpLs7u0Nkk14ak/w832NnRJ2KCXsUErYoZSwU3SDTtmUiZLlK8olOzZ3qGRnanUAiKIoqhqi+a4Z4SGi0D49XSnZq28tkGzv9K2StfeeCd5zbec8yUbv8vk5JXZK2KGUsEMpYYdSwk4q7g+Gup15nr33jmDetKlZsknl+rTDUW8+K9k9r62SLH0q3lBSefRcMK/o0gEov+9grHteKc48RyJRStihlLBDKWGHUsJOYh8zVhw6Fsy/+3uMZJPKO/t6OVEURVHD8VrJ2v7S711uGr8j+PmenE7VI9/8+soXdh6fB4ph7JSwQylhh1LCDqWEncQ+Zvw/mSWzJDu5QL8PWfbDYMkOLFsX++80dt0m2bdzdajJnuiRLD9rRvCeHSs1q150IPaakoDHjEgkSgk7lBJ2KCXsFN2gE1I2/AbJst0Zydq36vASRVH045yNkt31ygrJRjQX/ulLsWHQQSJRStihlLBDKWEnsV9duxTZru5Y1/WejP+C2dTFP0n2x/oyvTCnL4PhwtgpYYdSwg6lhB1KCTuUEnZKYvqOa8rq1mC+ZPp9kr0z9kvJ5tY/IVnltv45zD7J2Clhh1LCDqWEHUoJOww6/xF6ySuKoqh76RTJft2pZzw+07hFsmcfeUiy/PdDgn9nzMu7NYz3ddeiwk4JO5QSdigl7FBK2CmJF8f6QuZRPYnj3edfl6w6PTD2PaduWS7ZxA3HJTvX1hH7nm54cQyJRClhh1LCDqWEHQadAsrPrpHs+rVHJHvvls9j33PyV49JduuL+uQp+0tb7Hv2JwYdJBKlhB1KCTuUEnYYdPpY2cgRkh1bOCF47d7VTZINCOwbi9vnS9ZTF+8UkP7GoINEopSwQylhh1LCDqWEHaZvIx8c0RfHBqX0zMzT+bOSPbBilX72o70FWVchMX0jkSgl7FBK2KGUsMMJGQWUq6uR7HC9vjg2raYj+PnQUBOyLnO7fvbjfbE+mwTslLBDKWGHUsIOpYQdBp0YUjOnSda6UoeSDbM3SzZnoD59uRT/5Hsl25Op1gtzepJGUrFTwg6lhB1KCTuUEnZKdtBJV4+V7PCS0cFrX1j4vmQPD+4q+JrWdM6UrKWpVrJhmwNnoxcRdkrYoZSwQylhh1LCTtENOulxN0vWc+coyRa+9Jlkjw/9sODraTiug8rut3WgiaIoqtr0jWTDcsU91ISwU8IOpYQdSgk7lBJ2KCXsJGL6To+6SbLMxuuC1y6tbpFsUWVnwde0/GidZPvX10g2fMdByar+LL2J+lKwU8IOpYQdSgk7lBJ2+nXQOXu/Pm47+2RGsjUTPpFs/rWnCr6ezuyZYD5nZ4Nkk5/7WbKqEzrA5K58WSWHnRJ2KCXsUErYoZSw06+DTseD+n+idfrFz8S+kOYT4yVratFf6EplU5JNbmwP3nNip54dnr2MtSEedkrYoZSwQylhh1LCDj/uhKuKH3dCIlFK2KGUsEMpYYdSwg6lhB1KCTuUEnYoJexQStihlLBDKWGHUsIOpYQdSgk7sb9PCVwt7JSwQylhh1LCDqWEHUoJO5QSdigl7FBK2KGUsPMv3WYo1nNaJtsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = train_images[0]\n",
    "image_pixels = image.reshape(28, 28)\n",
    "plt.subplot(131)\n",
    "plt.imshow(image_pixels)\n",
    "plt.axis('off')\n",
    "\n",
    "print(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 5923],\n",
       "       [   1, 6742],\n",
       "       [   2, 5958],\n",
       "       [   3, 6131],\n",
       "       [   4, 5842],\n",
       "       [   5, 5421],\n",
       "       [   6, 5918],\n",
       "       [   7, 6265],\n",
       "       [   8, 5851],\n",
       "       [   9, 5949]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(unique, counts) = np.unique(train_labels, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "frequencies\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes are pretty well-balanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Classification\n",
    "\n",
    "We will try using the architecture we learnt about last time - Artificial Neural Networks (ANNs)\n",
    "\n",
    "We will also look at a much more efficient archiecture: Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape(60000, 28, 28, 1)\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Network (ANN)\n",
    "\n",
    "This is the architecture we learnt about last lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = keras.models.Sequential()\n",
    "#Input layer\n",
    "ann.add(keras.layers.Flatten(input_shape=[28, 28, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden Layer 1\n",
    "ann.add(keras.layers.Dense(50, activation='relu'))\n",
    "\n",
    "#Hidden Layer 2\n",
    "ann.add(keras.layers.Dense(25, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer (with softmax)\n",
    "ann.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 50)                39250     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40785 (159.32 KB)\n",
      "Trainable params: 40785 (159.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2df7a0150>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(train_images, train_labels, epochs=20, verbose=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "This is the most commonly used architecture for image classification. \n",
    "\n",
    "Learn more about this architecture in this lesson's video: https://youtu.be/DbClQQZujxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential()\n",
    "cnn.add(keras.layers.Conv2D(4, (4, 4), strides=2, activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn.add(keras.layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 4)         68        \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 4)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 144)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68 (272.00 Byte)\n",
      "Trainable params: 68 (272.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.add(keras.layers.Flatten())\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 4)         68        \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 4)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 50)                7250      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_19 (Dense)            (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8853 (34.58 KB)\n",
      "Trainable params: 8853 (34.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.add(keras.layers.Dense(50, activation='relu'))\n",
    "cnn.add(keras.layers.Dense(25, activation='relu'))\n",
    "cnn.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of CNNs\n",
    "\n",
    "Despite having a more complex architecture = more complex patterns, the number of parameters in a CNN are fewer (almost one-fourth).\n",
    "\n",
    "|Architecture|HL 1|HL 2|Parameters|\n",
    "|------------|----|----|----------|\n",
    "|ANN|50|25|40785|\n",
    "|CNN|50|25|8853|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2dfa91450>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_images, train_labels, epochs=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"../backend/model/test-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"../backend/model/test-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 481us/step - loss: 0.1491 - accuracy: 0.9693\n",
      "[0.14907628297805786, 0.9692999720573425]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.evaluate(test_images, test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Models\n",
    "\n",
    "Since the problem is multi-class classification, we will only look at Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 372us/step - loss: 0.5015 - accuracy: 0.9444\n",
      "ANN Accuracy: 0.9444000124931335\n"
     ]
    }
   ],
   "source": [
    "ann_score = ann.evaluate(test_images, test_labels)\n",
    "print(f\"ANN Accuracy: {ann_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 461us/step - loss: 0.1491 - accuracy: 0.9693\n",
      "CNN Accuracy: 0.9692999720573425\n"
     ]
    }
   ],
   "source": [
    "cnn_score = cnn.evaluate(test_images, test_labels)\n",
    "print(f\"CNN Accuracy: {cnn_score[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
